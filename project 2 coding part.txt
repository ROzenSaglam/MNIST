ans ;-1
def closed_form(X, Y, lambda_factor):
    """
    Computes the closed form solution of linear regression with L2 regularization

    Args:
        X - (n, d + 1) NumPy array (n datapoints each with d features plus the bias feature in the first dimension)
        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each
            data point
        lambda_factor - the regularization constant (scalar)
    Returns:
        theta - (d + 1, ) NumPy array containing the weights of linear regression. Note that theta[0]
        represents the y-axis intercept of the model and therefore X[0] = 1
    
    
    Computes the closed form solution of linear regression with L2 regularization
    Args:
        X - (n, d + 1) NumPy array (n datapoints each with d features plus the bias feature in the first dimension)
        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each
            data point
        lambda_factor - the regularization constant (scalar)
    Returns:
        theta - (d + 1, ) NumPy array containing the weights of linear regression. Note that theta[0]
        represents the y-axis intercept of the model and therefore X[0] = 1
    """
    return np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X), X) + lambda_factor*np.identity(X.shape[1])), np.transpose(X)), Y)

    raise NotImplementedError






ans:2

def one_vs_rest_svm(train_x, train_y, test_x):
    """
    Trains a linear SVM for binary classifciation

    Args:
        train_x - (n, d) NumPy array (n datapoints each with d features)
        train_y - (n, ) NumPy array containing the labels (0 or 1) for each training data point
        test_x - (m, d) NumPy array (m datapoints each with d features)
    Returns:
        pred_test_y - (m,) NumPy array containing the labels (0 or 1) for each test data point
    
     
    Trains a linear SVM for binary classifciation
    Args:
        train_x - (n, d) NumPy array (n datapoints each with d features)
        train_y - (n, ) NumPy array containing the labels (0 or 1) for each training data point
        test_x - (m, d) NumPy array (m datapoints each with d features)
    Returns:
        pred_test_y - (m,) NumPy array containing the labels (0 or 1) for each test data point
    """
    clf = LinearSVC(random_state=0, C=0.1)
    clf.fit(train_x, train_y)
    return clf.predict(test_x)

    raise NotImplementedError
ans:3


def multi_class_svm(train_x, train_y, test_x):
    """
    Trains a linear SVM for multiclass classifciation using a one-vs-rest strategy

    Args:
        train_x - (n, d) NumPy array (n datapoints each with d features)
        train_y - (n, ) NumPy array containing the labels (int) for each training data point
        test_x - (m, d) NumPy array (m datapoints each with d features)
    Returns:
        pred_test_y - (m,) NumPy array containing the labels (int) for each test data point
    """
    clf = LinearSVC(random_state=0, C=0.1)
    clf.fit(train_x, train_y)
    return clf.predict(test_x)
    raise NotImplementedError
ans4:
def compute_probabilities(X, theta, temp_parameter):
    """
    Computes, for each datapoint X[i], the probability that X[i] is labeled as j
    for j = 0, 1, ..., k-1
    Args:
        X - (n, d) NumPy array (n datapoints each with d features)
        theta - (k, d) NumPy array, where row j represents the parameters of our model for label j
        temp_parameter - the temperature parameter of softmax function (scalar)
    Returns:
        H - (k, n) NumPy array, where each entry H[j][i] is the probability that X[i] is labeled as j
    """
    H = np.zeros([theta.shape[0], X.shape[0]])
    exp = np.dot(theta, X.T) / temp_parameter
    c = np.max(exp, axis=0)
    H = np.exp(exp - c)
    H = H / np.sum(H, axis=0)
    return H
    raise NotImplementedError

ans5:

def compute_cost_function(X, Y, theta, lambda_factor, temp_parameter):
    """
    Computes the total cost over every datapoint.
    Args:
        X - (n, d) NumPy array (n datapoints each with d features)
        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each
            data point
        theta - (k, d) NumPy array, where row j represents the parameters of our
                model for label j
        lambda_factor - the regularization constant (scalar)
        temp_parameter - the temperature parameter of softmax function (scalar)
    Returns
        c - the cost value (scalar)
    """
    reg = lambda_factor / 2 * np.sum(theta*theta)
    p = compute_probabilities(X, theta, temp_parameter)
    sum = 0
    for i in np.arange(X.shape[0]):
        sum += np.log(p[Y[i], i])
    loss = -sum / X.shape[0]
    return loss + reg

    raise NotImplementedError

ans6:
def run_gradient_descent_iteration(X, Y, theta, alpha, lambda_factor, temp_parameter):
    """
    Runs one step of batch gradient descent
    Args:
        X - (n, d) NumPy array (n datapoints each with d features)
        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each
            data point
        theta - (k, d) NumPy array, where row j represents the parameters of our
                model for label j
        alpha - the learning rate (scalar)
        lambda_factor - the regularization constant (scalar)
        temp_parameter - the temperature parameter of softmax function (scalar)
    Returns:
        theta - (k, d) NumPy array that is the final value of parameters theta
    """
    probabilities = compute_probabilities(X, theta, temp_parameter)

    label_matrix = sparse.coo_matrix((np.ones(Y.shape[0]), (Y, np.arange(Y.shape[0]))), shape=(theta.shape[0], X.shape[0])).toarray()

    term1 = -1 / temp_parameter / X.shape[0] * np.dot(X.T, (label_matrix - probabilities).T).T
    gradient = term1 + lambda_factor*theta
    return theta - alpha*gradient
    raise NotImplementedError

ans:-7

def update_y(train_y, test_y):
    """
    Changes the old digit labels for the training and test set for the new (mod 3)
    labels.
    Args:
        train_y - (n, ) NumPy array containing the labels (a number between 0-9)
                 for each datapoint in the training set
        test_y - (n, ) NumPy array containing the labels (a number between 0-9)
                for each datapoint in the test set
    Returns:
        train_y_mod3 - (n, ) NumPy array containing the new labels (a number between 0-2)
                     for each datapoint in the training set
        test_y_mod3 - (n, ) NumPy array containing the new labels (a number between 0-2)
                    for each datapoint in the test set
    """
    return np.mod(train_y, 3), np.mod(test_y, 3)
    raise NotImplementedError

ans:-8
def compute_test_error_mod3(X, Y, theta, temp_parameter):
    """
    Returns the error of these new labels when the classifier predicts the digit. (mod 3)
    Args:
        X - (n, d - 1) NumPy array (n datapoints each with d - 1 features)
        Y - (n, ) NumPy array containing the labels (a number from 0-2) for each
            data point
        theta - (k, d) NumPy array, where row j represents the parameters of our
                model for label j
        temp_parameter - the temperature parameter of softmax function (scalar)
    Returns:
        test_error - the error rate of the classifier (scalar)
    """
    classification = get_classification(X, theta, temp_parameter)
    return 1 - np.mean(np.mod(classification, 3) == Y)
    raise NotImplementedError


ans:-9
def project_onto_PC(X, pcs, n_components, feature_means):
    """
    Given principal component vectors pcs = principal_components(X)
    this function returns a new data array in which each sample in X
    has been projected onto the first n_components principcal components.
    """
    # TODO: first center data using the feature_means
    # TODO: Return the projection of the centered dataset
    #       on the first n_components principal components.
    #       This should be an array with dimensions: n x n_components.
    # Hint: these principal components = first n_components columns
    #       of the eigenvectors returned by principal_components().
    #       Note that each eigenvector is already be a unit-vector,
    #       so the projection may be done using matrix multiplication.
    X_tild = X - feature_means
    pcs_local = pcs[0:pcs.shape[0], 0:n_components]
    return np.dot(X_tild, pcs_local)

    raise NotImplementedError

ans:10

def polynomial_kernel(X, Y, c, p):
    """
        Compute the polynomial kernel between two matrices X and Y::
            K(x, y) = (<x, y> + c)^p
        for each pair of rows x in X and y in Y.
        Args:
            X - (n, d) NumPy array (n datapoints each with d features)
            Y - (m, d) NumPy array (m datapoints each with d features)
            c - a coefficient to trade off high-order and low-order terms (scalar)
            p - the degree of the polynomial kernel
        Returns:
            kernel_matrix - (n, m) Numpy array containing the kernel matrix
    """
    # Compute dot product between each pair of rows in X and Y
    dot_products = np.dot(X, Y.T)
    
    # Compute kernel matrix using the polynomial kernel function
    kernel_matrix = (dot_products + c) ** p
    
    return kernel_matrix
    raise NotImplementedError


ans:11


import numpy as np

def rbf_kernel(X, Y, gamma):
    """
    Compute the Gaussian RBF kernel between two matrices X and Y:
    K(x, y) = exp(-gamma ||x-y||^2)
    for each pair of rows x in X and y in Y.

    Args:
    X - (n, d) NumPy array (n datapoints each with d features)
    Y - (m, d) NumPy array (m datapoints each with d features)
    gamma - the gamma parameter of gaussian function (scalar)

    Returns:
    kernel_matrix - (n, m) Numpy array containing the kernel matrix
    """
    n, d = X.shape
    m, d = Y.shape
    kernel_matrix = np.zeros((n, m))

    for i in range(n):
        for j in range(m):
            kernel_matrix[i, j] = np.exp(-gamma * np.linalg.norm(X[i] - Y[j]) ** 2)

    return kernel_matrix
    raise NotImplementedError


